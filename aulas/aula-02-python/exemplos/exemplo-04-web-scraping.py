"""
============================================
EXEMPLO 4: Web Scraping com BeautifulSoup
============================================
Conceito: Extrair dados de p√°ginas web
Pergunta: Como coletar dados de sites que n√£o t√™m API?

NESTE EXEMPLO VOC√ä APRENDE:
- Como fazer scraping de p√°ginas HTML
- Como usar BeautifulSoup para parsear HTML
- Como extrair dados espec√≠ficos de elementos
- Como tratar erros e casos especiais
- Exemplo pr√°tico: Mercado Livre

CASO DE NEG√ìCIO:
Coletar pre√ßos de produtos de sites de concorrentes
para an√°lise competitiva de pre√ßos quando n√£o h√° API dispon√≠vel
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# ============================================
# POR QUE WEB SCRAPING?
# ============================================
# Nem todos os sites t√™m API dispon√≠vel
# Web scraping extrai dados diretamente do HTML
# √ötil para: pre√ßos de concorrentes, not√≠cias, dados p√∫blicos


def fazer_requisicao(url: str) -> str:
    """
    Faz requisi√ß√£o HTTP e retorna HTML como string
    
    Args:
        url: URL da p√°gina
        
    Returns:
        HTML da p√°gina como string
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    
    except Exception as e:
        print(f"‚ùå Erro ao fazer requisi√ß√£o: {e}")
        return None


def parsear_produto_mercadolivre(html: str) -> dict:
    """
    Extrai informa√ß√µes de produto do HTML do Mercado Livre
    
    Args:
        html: HTML da p√°gina do produto
        
    Returns:
        Dicion√°rio com informa√ß√µes do produto
    """
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extrair nome do produto
        nome_element = soup.find('h1', class_='ui-pdp-title')
        nome = nome_element.get_text(strip=True) if nome_element else "N√£o encontrado"
        
        # Extrair pre√ßos
        preco_elements = soup.find_all('span', class_='andes-money-amount__fraction')
        
        if len(preco_elements) >= 3:
            preco_antigo = int(preco_elements[0].get_text(strip=True).replace('.', ''))
            preco_atual = int(preco_elements[1].get_text(strip=True).replace('.', ''))
            preco_parcela = int(preco_elements[2].get_text(strip=True).replace('.', ''))
        else:
            # Se n√£o encontrar 3 pre√ßos, tenta encontrar pelo menos 1
            preco_antigo = None
            preco_atual = int(preco_elements[0].get_text(strip=True).replace('.', '')) if preco_elements else None
            preco_parcela = None
        
        return {
            'nome_produto': nome,
            'preco_antigo': preco_antigo,
            'preco_atual': preco_atual,
            'preco_parcela': preco_parcela,
            'timestamp': pd.Timestamp.now().isoformat()
        }
    
    except Exception as e:
        print(f"‚ùå Erro ao parsear HTML: {e}")
        return None


# ============================================
# EXEMPLO 1: Scraping B√°sico
# ============================================

print("=" * 50)
print("EXEMPLO 1: Scraping B√°sico")
print("=" * 50)

# P√°gina simples de exemplo
url_exemplo = "https://example.com"
html = fazer_requisicao(url_exemplo)

if html:
    soup = BeautifulSoup(html, 'html.parser')
    titulo = soup.find('h1')
    if titulo:
        print(f"‚úÖ T√≠tulo encontrado: {titulo.get_text()}")


# ============================================
# EXEMPLO 2: Scraping Mercado Livre (Produto)
# ============================================

print("\n" + "=" * 50)
print("EXEMPLO 2: Scraping Mercado Livre - Produto")
print("=" * 50)

# URL de exemplo do Mercado Livre
# NOTA: URLs do Mercado Livre podem mudar, este √© um exemplo
url_ml = "https://www.mercadolivre.com.br/apple-iphone-16-pro-1-tb-titnio-preto-distribuidor-autorizado/p/MLB1040287851"

print(f"Buscando dados de: {url_ml}")

# Fazer requisi√ß√£o
html_ml = fazer_requisicao(url_ml)

if html_ml:
    # Parsear dados do produto
    dados_produto = parsear_produto_mercadolivre(html_ml)
    
    if dados_produto:
        print(f"\n‚úÖ Dados extra√≠dos:")
        print(f"   Nome: {dados_produto['nome_produto']}")
        if dados_produto['preco_antigo']:
            print(f"   Pre√ßo antigo: R$ {dados_produto['preco_antigo']:,.2f}")
        if dados_produto['preco_atual']:
            print(f"   Pre√ßo atual: R$ {dados_produto['preco_atual']:,.2f}")
        if dados_produto['preco_parcela']:
            print(f"   Pre√ßo parcelado: R$ {dados_produto['preco_parcela']:,.2f}")
        
        # Converter para DataFrame
        df_produto = pd.DataFrame([dados_produto])
        print(f"\nDataFrame criado:")
        print(df_produto)
    else:
        print("‚ö†Ô∏è N√£o foi poss√≠vel extrair dados. A estrutura do site pode ter mudado.")
else:
    print("‚ö†Ô∏è N√£o foi poss√≠vel acessar a p√°gina. Verifique a URL ou sua conex√£o.")


# ============================================
# EXEMPLO 3: Scraping com Seletores CSS
# ============================================

print("\n" + "=" * 50)
print("EXEMPLO 3: Usando Seletores CSS")
print("=" * 50)

# Exemplo de como usar seletores CSS
if html_ml:
    soup = BeautifulSoup(html_ml, 'html.parser')
    
    # Buscar elementos usando seletores CSS
    # Exemplo: buscar todos os links
    links = soup.select('a[href]')
    print(f"‚úÖ Encontrados {len(links)} links na p√°gina")
    
    # Buscar elementos espec√≠ficos
    # Exemplo: buscar imagens
    imagens = soup.select('img[src]')
    print(f"‚úÖ Encontradas {len(imagens)} imagens na p√°gina")


# ============================================
# BOAS PR√ÅTICAS
# ============================================

print("\n" + "=" * 50)
print("‚ö†Ô∏è BOAS PR√ÅTICAS DE WEB SCRAPING")
print("=" * 50)
print("""
1. ‚úÖ Sempre verifique os termos de uso do site
2. ‚úÖ Use delays entre requisi√ß√µes (time.sleep())
3. ‚úÖ Respeite robots.txt
4. ‚úÖ Use headers apropriados (User-Agent)
5. ‚úÖ Implemente tratamento de erros robusto
6. ‚úÖ Prefira APIs quando dispon√≠veis (mais confi√°vel)
7. ‚úÖ N√£o sobrecarregue o servidor com muitas requisi√ß√µes
8. ‚úÖ Sites podem mudar estrutura - c√≥digo pode quebrar
""")


# ============================================
# COMPARA√á√ÉO: API vs WEB SCRAPING
# ============================================

print("\n" + "=" * 50)
print("üí° API vs WEB SCRAPING")
print("=" * 50)
print("""
API (Application Programming Interface):
‚úÖ Forma oficial de acessar dados
‚úÖ Estruturado e confi√°vel
‚úÖ Documenta√ß√£o dispon√≠vel
‚úÖ Mais r√°pido e eficiente
‚úÖ Menos chance de quebrar

WEB SCRAPING:
‚úÖ √ötil quando n√£o h√° API
‚úÖ Acessa dados p√∫blicos
‚úÖ Pode quebrar se site mudar
‚úÖ Mais lento (precisa parsear HTML)
‚úÖ Pode violar termos de uso

RECOMENDA√á√ÉO: Use API quando poss√≠vel, scraping como √∫ltimo recurso.
""")
