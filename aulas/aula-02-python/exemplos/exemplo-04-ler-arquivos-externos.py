"""
============================================
EXEMPLO 4: Conectar com DataLake e Ler Parquet
============================================
Conceito: Conectar com DataLake usando boto3 e ler arquivos Parquet
Pergunta: Como ler dados de um DataLake usando a API S3?

NESTE EXEMPLO VOC√ä APRENDE:
- Como conectar com DataLake usando boto3
- Como ler arquivos Parquet de um DataLake
- Por que DataLakes s√£o importantes na ind√∫stria
- Como trabalhar com Supabase Storage (compat√≠vel com S3)

CASO DE NEG√ìCIO:
Ler dados de pre√ßos de concorrentes armazenados em um DataLake
para an√°lise competitiva e tomada de decis√£o
"""

import pandas as pd
import boto3
import io

# ============================================
# POR QUE DATA LAKES S√ÉO IMPORTANTES?
# ============================================
"""
Data Lakes s√£o reposit√≥rios centralizados que armazenam dados em seu formato bruto.
S√£o amplamente utilizados na ind√∫stria para:

‚úÖ Armazenar grandes volumes de dados (terabytes/petabytes)
‚úÖ Manter dados em formato original (sem transforma√ß√£o pr√©via)
‚úÖ Suportar m√∫ltiplos formatos (CSV, Parquet, JSON, etc.)
‚úÖ Escalabilidade horizontal (cresce conforme necessidade)
‚úÖ Economia de custos (armazenamento barato)

üåç AWS S3 √â O PADR√ÉO DA IND√öSTRIA:
- Mais de 50% das empresas usam AWS S3 para Data Lakes
- API padr√£o que funciona com m√∫ltiplas ferramentas
- Compat√≠vel com Supabase Storage, MinIO, e outros

üìä CASO DE USO REAL:
Empresas armazenam dados de vendas, produtos, pre√ßos de concorrentes
em Data Lakes para an√°lises e machine learning.
"""

# ============================================
# CONFIGURA√á√ïES DO DATA LAKE
# ============================================

# Supabase Storage usa API compat√≠vel com S3
# Isso permite usar boto3 (biblioteca padr√£o da AWS)

S3_ENDPOINT_URL = "https://zsutlhnykwxackvunyvr.storage.supabase.co/storage/v1/s3"
AWS_REGION = "us-west-2"

# Credenciais (em produ√ß√£o, use vari√°veis de ambiente!)
AWS_ACCESS_KEY_ID = "24f38596737f3de9352bdfbb86b2493f"
AWS_SECRET_ACCESS_KEY = "e3f46aac4d7db5d69a173f40d0f65c1457fce3b81d483f0201ec22e63329520e"

BUCKET_NAME = "meu_bucket"
FILE_KEY = "preco_competidores.parquet"  # Arquivo Parquet no bucket

# ============================================
# PASSO 1: Conectar com DataLake
# ============================================

print("=" * 50)
print("PASSO 1: Conectando com DataLake")
print("=" * 50)

print(f"üîó Conectando ao Data Lake (Supabase Storage)...")
print(f"   Endpoint: {S3_ENDPOINT_URL}")
print(f"   Bucket: {BUCKET_NAME}")
print(f"   Arquivo: {FILE_KEY}")

# Criar cliente S3
# boto3 √© a biblioteca padr√£o da AWS para trabalhar com S3
# Funciona tamb√©m com Supabase Storage (API compat√≠vel)
s3 = boto3.client(
    "s3",
    region_name=AWS_REGION,
    endpoint_url=S3_ENDPOINT_URL,
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)

print("‚úÖ Cliente S3 criado com sucesso!")

# ============================================
# PASSO 2: Baixar Arquivo Parquet do DataLake
# ============================================

print("\n" + "=" * 50)
print("PASSO 2: Baixando Arquivo Parquet do DataLake")
print("=" * 50)

try:
    # Baixar arquivo do Data Lake
    print(f"üì• Baixando arquivo Parquet do Data Lake...")
    response = s3.get_object(
        Bucket=BUCKET_NAME,
        Key=FILE_KEY
    )
    
    # Ler bytes do arquivo Parquet
    parquet_bytes = response["Body"].read()
    print(f"‚úÖ Arquivo baixado: {len(parquet_bytes):,} bytes")
    
except Exception as e:
    print(f"‚ùå Erro ao baixar arquivo: {e}")
    print("\nüí° Verifique:")
    print("   - Se o bucket existe")
    print("   - Se o arquivo est√° no bucket")
    print("   - Se as credenciais est√£o corretas")
    exit(1)

# ============================================
# PASSO 3: Converter Parquet para DataFrame
# ============================================

print("\n" + "=" * 50)
print("PASSO 3: Convertendo Parquet para DataFrame")
print("=" * 50)

try:
    # Converter bytes do Parquet para DataFrame
    # Parquet √© um formato bin√°rio otimizado para Big Data
    df_precos = pd.read_parquet(io.BytesIO(parquet_bytes))
    
    print(f"‚úÖ Parquet convertido para DataFrame com sucesso!")
    print(f"   Linhas x Colunas: {df_precos.shape}")
    
except Exception as e:
    print(f"‚ùå Erro ao converter Parquet: {e}")
    print("\nüí° Verifique:")
    print("   - Se o arquivo √© realmente um Parquet v√°lido")
    print("   - Se o pyarrow est√° instalado: pip install pyarrow")
    exit(1)

# ============================================
# PASSO 4: Explorar e Analisar Dados
# ============================================

print("\n" + "=" * 50)
print("PASSO 4: Explorando Dados")
print("=" * 50)

# Visualizar primeiras linhas
print(f"\nüìä Primeiras linhas:")
print(df_precos.head())

# Informa√ß√µes do DataFrame
print(f"\nüìã Informa√ß√µes do DataFrame:")
print(df_precos.info())

# Estat√≠sticas descritivas
print(f"\nüìà Estat√≠sticas de pre√ßo:")
print(df_precos["preco_concorrente"].describe())

# An√°lise de concorrentes
print(f"\nüè™ Concorrentes:")
print(df_precos["nome_concorrente"].value_counts())

# Resumo
print(f"\nüí° Resumo dos dados:")
print(f"   - Total de registros: {len(df_precos):,}")
print(f"   - Concorrentes √∫nicos: {df_precos['nome_concorrente'].nunique()}")
print(f"   - Produtos √∫nicos: {df_precos['id_produto'].nunique()}")
print(f"   - Pre√ßo m√©dio: R$ {df_precos['preco_concorrente'].mean():.2f}")
print(f"   - Pre√ßo m√≠nimo: R$ {df_precos['preco_concorrente'].min():.2f}")
print(f"   - Pre√ßo m√°ximo: R$ {df_precos['preco_concorrente'].max():.2f}")

# ============================================
# RESUMO: Por que Parquet?
# ============================================

print("\n" + "=" * 50)
print("üí° POR QUE PARQUET √â IDEAL PARA DATA LAKES?")
print("=" * 50)
print("""
‚úÖ Compress√£o eficiente:
   - Arquivos 50-90% menores que CSV
   - Economia de espa√ßo e custos

‚úÖ Performance superior:
   - Leitura mais r√°pida, especialmente para grandes volumes
   - Formato columnar otimizado para an√°lises

‚úÖ Schema embutido:
   - Preserva tipos de dados automaticamente
   - N√£o precisa inferir tipos ao ler

‚úÖ Ideal para Big Data:
   - Suportado por Spark, Pandas, Dask, etc.
   - Otimizado para processamento distribu√≠do

‚úÖ Predicate pushdown:
   - L√™ apenas colunas necess√°rias
   - Reduz I/O e melhora performance
""")

print("=" * 50)
print("‚úÖ Dados do DataLake carregados e prontos para an√°lise!")
print("=" * 50)
